transformer.wte.weight

transformer.wpe.weight

transformer.h.0.ln_1.weight

transformer.h.0.ln_1.bias

transformer.h.0.attn.attention.k_proj.weight

transformer.h.0.attn.attention.v_proj.weight

transformer.h.0.attn.attention.q_proj.weight

transformer.h.0.attn.attention.out_proj.weight

transformer.h.0.attn.attention.out_proj.bias

transformer.h.0.ln_2.weight

transformer.h.0.ln_2.bias

transformer.h.0.mlp.c_fc.weight

transformer.h.0.mlp.c_fc.bias

transformer.h.0.mlp.c_proj.weight

transformer.h.0.mlp.c_proj.bias

transformer.h.1.ln_1.weight

transformer.h.1.ln_1.bias

transformer.h.1.attn.attention.k_proj.weight

transformer.h.1.attn.attention.v_proj.weight

transformer.h.1.attn.attention.q_proj.weight

transformer.h.1.attn.attention.out_proj.weight

transformer.h.1.attn.attention.out_proj.bias

transformer.h.1.ln_2.weight

transformer.h.1.ln_2.bias

transformer.h.1.mlp.c_fc.weight

transformer.h.1.mlp.c_fc.bias

transformer.h.1.mlp.c_proj.weight

transformer.h.1.mlp.c_proj.bias

transformer.h.2.ln_1.weight

transformer.h.2.ln_1.bias

transformer.h.2.attn.attention.k_proj.weight

transformer.h.2.attn.attention.v_proj.weight

transformer.h.2.attn.attention.q_proj.weight

transformer.h.2.attn.attention.out_proj.weight

transformer.h.2.attn.attention.out_proj.bias

transformer.h.2.ln_2.weight

transformer.h.2.ln_2.bias

transformer.h.2.mlp.c_fc.weight

transformer.h.2.mlp.c_fc.bias

transformer.h.2.mlp.c_proj.weight

transformer.h.2.mlp.c_proj.bias

transformer.h.3.ln_1.weight

transformer.h.3.ln_1.bias

transformer.h.3.attn.attention.k_proj.weight

transformer.h.3.attn.attention.v_proj.weight

transformer.h.3.attn.attention.q_proj.weight

transformer.h.3.attn.attention.out_proj.weight

transformer.h.3.attn.attention.out_proj.bias

transformer.h.3.ln_2.weight

transformer.h.3.ln_2.bias

transformer.h.3.mlp.c_fc.weight

transformer.h.3.mlp.c_fc.bias

transformer.h.3.mlp.c_proj.weight

transformer.h.3.mlp.c_proj.bias

transformer.h.4.ln_1.weight

transformer.h.4.ln_1.bias

transformer.h.4.attn.attention.k_proj.weight

transformer.h.4.attn.attention.v_proj.weight

transformer.h.4.attn.attention.q_proj.weight

transformer.h.4.attn.attention.out_proj.weight

transformer.h.4.attn.attention.out_proj.bias

transformer.h.4.ln_2.weight

transformer.h.4.ln_2.bias

transformer.h.4.mlp.c_fc.weight

transformer.h.4.mlp.c_fc.bias

transformer.h.4.mlp.c_proj.weight

transformer.h.4.mlp.c_proj.bias

transformer.h.5.ln_1.weight

transformer.h.5.ln_1.bias

transformer.h.5.attn.attention.k_proj.weight

transformer.h.5.attn.attention.v_proj.weight

transformer.h.5.attn.attention.q_proj.weight

transformer.h.5.attn.attention.out_proj.weight

transformer.h.5.attn.attention.out_proj.bias

transformer.h.5.ln_2.weight

transformer.h.5.ln_2.bias

transformer.h.5.mlp.c_fc.weight

transformer.h.5.mlp.c_fc.bias

transformer.h.5.mlp.c_proj.weight

transformer.h.5.mlp.c_proj.bias

transformer.h.6.ln_1.weight

transformer.h.6.ln_1.bias

transformer.h.6.attn.attention.k_proj.weight

transformer.h.6.attn.attention.v_proj.weight

transformer.h.6.attn.attention.q_proj.weight

transformer.h.6.attn.attention.out_proj.weight

transformer.h.6.attn.attention.out_proj.bias

transformer.h.6.ln_2.weight

transformer.h.6.ln_2.bias

transformer.h.6.mlp.c_fc.weight

transformer.h.6.mlp.c_fc.bias

transformer.h.6.mlp.c_proj.weight

transformer.h.6.mlp.c_proj.bias

transformer.h.7.ln_1.weight

transformer.h.7.ln_1.bias

transformer.h.7.attn.attention.k_proj.weight

transformer.h.7.attn.attention.v_proj.weight

transformer.h.7.attn.attention.q_proj.weight

transformer.h.7.attn.attention.out_proj.weight

transformer.h.7.attn.attention.out_proj.bias

transformer.h.7.ln_2.weight

transformer.h.7.ln_2.bias

transformer.h.7.mlp.c_fc.weight

transformer.h.7.mlp.c_fc.bias

transformer.h.7.mlp.c_proj.weight

transformer.h.7.mlp.c_proj.bias

transformer.h.8.ln_1.weight

transformer.h.8.ln_1.bias

transformer.h.8.attn.attention.k_proj.weight

transformer.h.8.attn.attention.v_proj.weight

transformer.h.8.attn.attention.q_proj.weight

transformer.h.8.attn.attention.out_proj.weight

transformer.h.8.attn.attention.out_proj.bias

transformer.h.8.ln_2.weight

transformer.h.8.ln_2.bias

transformer.h.8.mlp.c_fc.weight

transformer.h.8.mlp.c_fc.bias

transformer.h.8.mlp.c_proj.weight

transformer.h.8.mlp.c_proj.bias

transformer.h.9.ln_1.weight

transformer.h.9.ln_1.bias

transformer.h.9.attn.attention.k_proj.weight

transformer.h.9.attn.attention.v_proj.weight

transformer.h.9.attn.attention.q_proj.weight

transformer.h.9.attn.attention.out_proj.weight

transformer.h.9.attn.attention.out_proj.bias

transformer.h.9.ln_2.weight

transformer.h.9.ln_2.bias

transformer.h.9.mlp.c_fc.weight

transformer.h.9.mlp.c_fc.bias

transformer.h.9.mlp.c_proj.weight

transformer.h.9.mlp.c_proj.bias

transformer.h.10.ln_1.weight

transformer.h.10.ln_1.bias

transformer.h.10.attn.attention.k_proj.weight

transformer.h.10.attn.attention.v_proj.weight

transformer.h.10.attn.attention.q_proj.weight

transformer.h.10.attn.attention.out_proj.weight

transformer.h.10.attn.attention.out_proj.bias

transformer.h.10.ln_2.weight

transformer.h.10.ln_2.bias

transformer.h.10.mlp.c_fc.weight

transformer.h.10.mlp.c_fc.bias

transformer.h.10.mlp.c_proj.weight

transformer.h.10.mlp.c_proj.bias

transformer.h.11.ln_1.weight

transformer.h.11.ln_1.bias

transformer.h.11.attn.attention.k_proj.weight

transformer.h.11.attn.attention.v_proj.weight

transformer.h.11.attn.attention.q_proj.weight

transformer.h.11.attn.attention.out_proj.weight

transformer.h.11.attn.attention.out_proj.bias

transformer.h.11.ln_2.weight

transformer.h.11.ln_2.bias

transformer.h.11.mlp.c_fc.weight

transformer.h.11.mlp.c_fc.bias

transformer.h.11.mlp.c_proj.weight

transformer.h.11.mlp.c_proj.bias

transformer.ln_f.weight

transformer.ln_f.bias

lm_head.weight

from layers.weight_gin_bbaiter import gin_bbai
model = gin_bbai().to('cuda')